{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"mass",
				"mass_query\tfunction"
			],
			[
				"resu",
				"result"
			],
			[
				"in",
				"info"
			],
			[
				"mass_query_co",
				"mass_query_covariance\tfunction"
			],
			[
				"prop",
				"proportion_to_show"
			],
			[
				"data_pro",
				"data_proportions_to_plot"
			],
			[
				"data_pr",
				"data_proportion_to_plot"
			],
			[
				"Da",
				"DataSource\tclass"
			],
			[
				"datas",
				"data_source\t(tools.py)"
			],
			[
				"min",
				"min_xvalue"
			],
			[
				"mat",
				"matplotlib_plot_2D_confidence_interval\tfunction"
			],
			[
				"num_of",
				"num_of_plot"
			],
			[
				"tree",
				"app_decision_tree"
			],
			[
				"app",
				"app_linear"
			],
			[
				"da",
				"data"
			],
			[
				"num",
				"num_training_points_model\tstatement"
			],
			[
				"varian",
				"variance_training_points_model\tstatement"
			],
			[
				"vai",
				"variance_training_points_model\tstatement"
			],
			[
				"ave",
				"averageX_training_points_model\tstatement"
			],
			[
				"training",
				"training_data_model"
			],
			[
				"dimen",
				"dimensionX\tstatement"
			],
			[
				"avera",
				"averageX_training_points_model"
			],
			[
				"query",
				"query2mysql\tfunction"
			],
			[
				"hive",
				"hivepassword"
			],
			[
				"dele",
				"deletable.log\t(logs.py)"
			],
			[
				"logg",
				"logger_file"
			],
			[
				"war",
				"warning\t(core.py)"
			],
			[
				"approximate_vari",
				"approximate_variance_y_from_to\tfunction"
			],
			[
				"x",
				"x_min"
			],
			[
				"ex",
				"exact_result\tstatement"
			],
			[
				"time",
				"time_ratio\tfunction"
			],
			[
				"approx",
				"approx_times"
			],
			[
				"appro",
				"approx_results"
			],
			[
				"exa",
				"exact_times"
			],
			[
				"approx_time",
				"approx_times"
			],
			[
				"exac",
				"exact_times"
			],
			[
				"date",
				"datetime"
			],
			[
				"lo",
				"logger\tstatement"
			],
			[
				"try",
				"try\tTry/Except/Finally"
			],
			[
				"rand",
				"randrange\tstatement"
			],
			[
				"Q",
				"Query_Engine_2d"
			],
			[
				"2d",
				"2d_query_engine"
			],
			[
				"allow",
				"b_allow_repeated_value"
			],
			[
				"re",
				"remove_repeated_x_1d\tfunction"
			],
			[
				"to",
				"to_csv\tfunction"
			],
			[
				"l",
				"labels\tstatement"
			],
			[
				"loa",
				"load2d"
			],
			[
				"defa",
				"default_logger_name"
			],
			[
				"qu",
				"query_2d"
			],
			[
				"if",
				"ifmain\tif __name__ == '__main__'"
			],
			[
				"nu",
				"num_training_points\tstatement"
			],
			[
				"set",
				"set_logging\tfunction"
			],
			[
				"plot",
				"plot_training_data_3d\tfunction"
			],
			[
				"training_da",
				"training_data"
			],
			[
				"plot_tr",
				"plot_training_data_2d\tfunction"
			],
			[
				"training_data",
				"training_data_model"
			],
			[
				"pl",
				"matplotlib_plot_2D\tfunction"
			],
			[
				"inte",
				"integrate\tmodule"
			],
			[
				"set_x",
				"set_xlabel"
			],
			[
				"mesh",
				"mesh_grid_num"
			],
			[
				"print",
				"print_function\tstatement"
			],
			[
				"test_density",
				"test_density_estimation_plt_2d"
			],
			[
				"den",
				"desngity_estimation_plt\tfunction"
			],
			[
				"log",
				"logger_object\t(core.py)"
			],
			[
				"data",
				"data_loader\t(test_query_engine.py)"
			],
			[
				"tru",
				"True\tinstance"
			],
			[
				"logger",
				"logger_object"
			],
			[
				"base",
				"base_models"
			],
			[
				"de",
				"density_estimation\tfunction"
			],
			[
				"Que",
				"QueryLogs"
			],
			[
				"De",
				"DEBUG\tstatement"
			],
			[
				"score",
				"score_samples\tfunction"
			],
			[
				"tesing",
				"testing_data"
			],
			[
				"split",
				"split_data_to_2\tfunction"
			],
			[
				"__",
				"__name__\t(core.py)"
			],
			[
				"trai",
				"training_data_model"
			],
			[
				"lab",
				"labels"
			],
			[
				"training_dat",
				"training_data_model"
			],
			[
				"num_",
				"num_total_training_points\tstatement"
			],
			[
				"C",
				"ClientClass"
			],
			[
				"Advance",
				"AdvancedTestSuite\t(test_advanced.py)"
			],
			[
				"flot",
				"float2str\t(Data.py)"
			],
			[
				"assertT",
				"assertTrue\tjunit method"
			],
			[
				"toS",
				"toString()\tfunc"
			],
			[
				"println",
				"println()\tfunc"
			],
			[
				"me",
				"messageUtil"
			],
			[
				"assert",
				"assertEquals\tjunit method"
			],
			[
				"Test",
				"Test\tjunit anno"
			],
			[
				"M",
				"MessageUtil\t(MessageUtil.java)"
			],
			[
				"TestU",
				"TestJunit\t(TestJunit.java)"
			],
			[
				"qin",
				"qingzhi.HelloWorld\t(build.xml)"
			],
			[
				"i-sca",
				"I-Scanner\tinit"
			],
			[
				"pub",
				"public"
			],
			[
				"test",
				"Test\tjunit anno"
			],
			[
				"H",
				"HelloWorld\t(HelloWorld.java)"
			],
			[
				"cla",
				"classes"
			],
			[
				"h",
				"HelloWorld"
			],
			[
				"sr",
				"src.dir"
			],
			[
				"buil",
				"build.dir"
			],
			[
				"bui",
				"build.dir"
			],
			[
				"pa",
				"path"
			],
			[
				"onmouseo",
				"onmouseover\tAttr"
			],
			[
				"getEl",
				"getElementById\t( elementId: DOMString ): Element Document"
			],
			[
				"getE",
				"getElementById\t( elementId: DOMString ): Element Document"
			],
			[
				"fun",
				"function"
			],
			[
				"s",
				"style"
			],
			[
				"d",
				"demo"
			],
			[
				"get",
				"getElementById"
			],
			[
				"t",
				"type\tAttr"
			],
			[
				"action",
				"action_page"
			],
			[
				"col",
				"color"
			],
			[
				"my",
				"myFunction"
			],
			[
				"co",
				"color"
			],
			[
				"back",
				"background-color"
			],
			[
				"list",
				"list-style-type"
			],
			[
				"he",
				"head2"
			],
			[
				"head",
				"head1"
			],
			[
				"font",
				"font-size"
			],
			[
				"ba",
				"background-color"
			],
			[
				"rgb",
				"rgba()"
			],
			[
				"bo",
				"border"
			],
			[
				"text",
				"text-align"
			],
			[
				"fon",
				"font-family"
			]
		]
	},
	"buffers":
	[
		{
			"contents": "xy\nprice_cost\nprice_cost_1t\nprice_cost_1t_sample_5m\nprice_cost_1t_sorted\nprice_cost_100k\nidx_ss_list_price\nstore_sales\nstore_sales_sample_1_percent\nstore_sales_sample_1_percent_cached\nstore_sales_sample_1m_cached",
			"settings":
			{
				"buffer_size": 212,
				"line_ending": "Unix",
				"name": "ðŒ† Outline",
				"scratch": true
			}
		},
		{
			"file": "creg/tools.py",
			"settings":
			{
				"buffer_size": 34640,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "creg/generate_random.py",
			"settings":
			{
				"buffer_size": 1544,
				"encoding": "UTF-8",
				"line_ending": "Unix",
				"name": "from __future__ import division"
			}
		},
		{
			"contents": "#!/usr/bin/env python\n# coding=utf-8\nfrom __future__ import print_function, division\nfrom core import CRegression\nimport tools\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KernelDensity\nimport evaluation\nimport logs\nimport data_loader as dl\nfrom scipy import integrate\nfrom datetime import datetime\nimport warnings\nimport generate_random\n\nepsabs = 1E-01\nepsrel = 1E-03\nmesh_grid_num = 30\nopts = {'epsabs': epsabs, 'epsrel': epsrel, 'limit': 100}\n# opts = {'epsabs': 1.49e-03, 'epsrel': 1.49e-03, 'limit': 100}\n\n# variable_names=[a1,b1,c1,d1,e1,f1,g1,h1,i1,j1,k1,l1,m1,n1,o1,p1,q1,r1,s1,t1,u1,v1,w1,x1,y1,z1,\n# a2,b2,c2,d2,e2,f2,g2,h2,i2,j2,k2,l2,m2,n2,o2,p2,q2,r2,s2,t2,u2,v2,w2,x2,y2,z2]\n\n\nclass QueryEngine:\n\n    \"\"\"Summary\n\n    Attributes:\n        cregression (TYPE): Description\n        dimension (TYPE): Description\n        kde (TYPE): Description\n        log_dens (TYPE): Description\n        logger (TYPE): Description\n        num_training_points (TYPE): Description\n        training_data (TYPE): Description\n    \"\"\"\n\n    def __init__(self, cregression, logger_object=None, b_print_time_cost=True, num_training_points=None):\n        if num_training_points is None:\n            self.num_training_points = cregression.num_total_training_points\n        else:\n            self.num_training_points = num_training_points\n        self.log_dens = None\n        self.training_data = cregression.training_data\n        self.kde = None  # kernel density object\n        self.dimension = self.training_data.features.shape[1]\n        self.cregression = cregression\n        if logger_object:\n            self.logger = logger_object.logger\n        else:\n            self.logger = logs.QueryLogs().logger\n        self.b_print_time_cost = b_print_time_cost\n\n        warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n    def density_estimation(self, kernel=None):\n        \"\"\"Estimate the density of points.\n\n        Args:\n            kernel (None, optional): Should be one of [â€˜gaussianâ€™,â€™tophatâ€™,â€™epanechnikovâ€™,â€™exponentialâ€™,â€™linearâ€™,â€™cosineâ€™]\n            Default is â€˜gaussianâ€™.\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        if kernel is None:\n            kernel = 'gaussian'\n        self.kde = KernelDensity(kernel=kernel).fit(\n            self.training_data.features)\n        return self.kde\n\n    def desngity_estimation_plt2d(self):\n        \"\"\" plot the density distribution\n\n        Returns:\n            TYPE: Ture if the plot is shown\n        \"\"\"\n        X_plot = np.linspace(min(self.training_data.features), max(\n            self.training_data.features), mesh_grid_num)[:, np.newaxis]\n        self.log_dens = self.kde.score_samples(X_plot)\n\n        ax = plt.subplot(111)\n\n        ax.plot(X_plot[:, 0], np.exp(self.log_dens), '-')\n        ax.plot(self.training_data.features, -0.001 - 0.001 *\n                np.random.random(self.training_data.features.shape[0]), '+k')\n        ax.set_xlabel(self.training_data.headers[0])\n        ax.set_ylabel(\"Probability\")\n        ax.set_title(\"2D Density Estimation\")\n        plt.show()\n        return True\n\n    def desngity_estimation_plt3d(self):\n        x = np.linspace(min(self.training_data.features[:, 0]), max(\n            self.training_data.features[:, 0]), mesh_grid_num)\n        y = np.linspace(min(self.training_data.features[:, 1]), max(\n            self.training_data.features[:, 1]), mesh_grid_num)\n        X, Y = np.meshgrid(x, y)\n\n        X1d = X.reshape(mesh_grid_num * mesh_grid_num)\n        Y1d = Y.reshape(mesh_grid_num * mesh_grid_num)\n        Z1d = [[X1d[i], Y1d[i]] for i in range(len(X1d))]\n        Z = self.kde.score_samples(Z1d)\n        Z_plot = Z.reshape((mesh_grid_num, mesh_grid_num))\n        # print(Z_plot)\n\n        self.log_dens = Z_plot\n\n        ax = plt.axes(projection='3d')\n\n        ax.plot_surface(X, Y, np.exp(Z_plot), rstride=1, cstride=1,\n                        cmap='viridis', edgecolor='none', alpha=0.8)\n        # ax.plot(self.training_data.features,-0.001- 0.001 * np.random.random(self.training_data.features.shape[0]), '+k')\n        ax.scatter(self.training_data.features[:, 0], self.training_data.features[:, 1], -0.0005 -\n                   0.0001 * np.random.random(self.training_data.features[:, 0].shape[0]), '+k', alpha=0.8, s=1)\n        ax.set_title(\"3D Density Estimation\")\n        ax.set_xlabel(self.training_data.headers[0])\n        ax.set_ylabel(self.training_data.headers[1])\n        ax.set_zlabel(\"Probability\")\n        plt.show()\n        return True\n\n    def approximate_avg_from_to(self, x_min, x_max, x_columnID):\n        \"\"\" calculate the approximate average value between x_min and x_max\n\n        Args:\n            x_min (TYPE): lower bound\n            x_max (TYPE): upper bound\n            x_columnID (TYPE): the index of the x to be interated\n\n        Returns:\n            TYPE: the integeral value\n        \"\"\"\n        start = datetime.now()\n        if self.dimension is 1:\n            def f_pRx(x):\n                # print(self.cregression.predict(x))\n                return np.exp(self.kde.score_samples(x)) * self.cregression.predict(x)\n\n            def f_p(x):\n                return np.exp(self.kde.score_samples(x))\n            a = integrate.quad(f_pRx, x_min, x_max,\n                               epsabs=epsabs, epsrel=epsrel)[0]\n            b = integrate.quad(f_p, x_min, x_max,\n                               epsabs=epsabs, epsrel=epsrel)[0]\n\n            if b:\n                result = a / b\n            else:\n                result = None\n\n        if self.dimension > 1:\n            data_range_length_half = [(max(self.training_data.features[:, i]) - min(\n                self.training_data.features[:, i])) * 0.5 for i in range(self.dimension)]\n            data_range = [[min(self.training_data.features[:, i]) - data_range_length_half[i], max(\n                self.training_data.features[:, i]) + data_range_length_half[i]] for i in range(self.dimension)]\n\n            # generate the integral bounds\n            bounds = []\n            for i in range(x_columnID):\n                bounds.append(data_range[i])\n            bounds.append([x_min, x_max])\n            # print(bounds)\n            for i in range(x_columnID + 1, self.dimension):\n                bounds.append(data_range[i])\n\n            def f_p(*args):\n                # print(np.exp(self.kde.score_samples(np.array(args).reshape(1,-1))))\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n\n            def f_pRx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args))\n            a = integrate.nquad(f_pRx, bounds, opts=opts)[0]\n            b = integrate.nquad(f_p, bounds, opts=opts)[0]\n\n            if b:\n                result = a / b\n            else:\n                result = None\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate AVG: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate AVG: %.4fs.\" % time_cost)\n        return result, time_cost\n\n    def approximate_sum_from_to(self, x_min, x_max, x_columnID):\n        start = datetime.now()\n        if self.dimension is 1:\n            # average = self.approximate_ave_from_to(x_min,x_max,x_columnID)\n            def f_pRx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args))\n            result = integrate.quad(f_pRx, x_min, x_max, epsabs=epsabs, epsrel=epsrel)[\n                0] * self.num_training_points\n            # return result\n\n        if self.dimension > 1:\n            data_range_length_half = [(max(self.training_data.features[:, i]) - min(\n                self.training_data.features[:, i])) * 0.5 for i in range(self.dimension)]\n            data_range = [[min(self.training_data.features[:, i]) - data_range_length_half[i], max(\n                self.training_data.features[:, i]) + data_range_length_half[i]] for i in range(self.dimension)]\n\n            # generate the integral bounds\n            bounds = []\n            for i in range(x_columnID):\n                bounds.append(data_range[i])\n            bounds.append([x_min, x_max])\n            # print(bounds)\n            for i in range(x_columnID + 1, self.dimension):\n                bounds.append(data_range[i])\n\n            def f_pRx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args))\n            result = integrate.nquad(f_pRx, bounds, opts=opts)[\n                0] * self.num_training_points\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate SUM: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate SUM: %.4fs.\" % time_cost)\n        return result, time_cost\n\n    def approximate_count_from_to(self, x_min, x_max, x_columnID):\n        start = datetime.now()\n        if self.dimension is 1:\n            # average = self.approximate_ave_from_to(x_min,x_max,x_columnID)\n            def f_p(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n            result = integrate.quad(f_p, x_min, x_max, epsabs=epsabs, epsrel=epsrel)[\n                0] * self.num_training_points\n            # return result\n\n        if self.dimension > 1:\n            data_range_length_half = [(max(self.training_data.features[:, i]) - min(\n                self.training_data.features[:, i])) * 0.5 for i in range(self.dimension)]\n            data_range = [[min(self.training_data.features[:, i]) - data_range_length_half[i], max(\n                self.training_data.features[:, i]) + data_range_length_half[i]] for i in range(self.dimension)]\n\n            # generate the integral bounds\n            bounds = []\n            for i in range(x_columnID):\n                bounds.append(data_range[i])\n            bounds.append([x_min, x_max])\n            # print(bounds)\n            for i in range(x_columnID + 1, self.dimension):\n                bounds.append(data_range[i])\n\n            def f_p(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n            result = integrate.nquad(f_p, bounds, opts=opts)[\n                0] * self.num_training_points\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate count: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate COUNT: %.4fs.\" % time_cost)\n        return int(result), time_cost\n\n    def approximate_variance_x_from_to(self, x_min=-np.inf, x_max=np.inf, x_columnID=0):\n        start = datetime.now()\n        if self.dimension is 1:\n            # average = self.approximate_ave_from_to(x_min,x_max,x_columnID)\n            def f_p(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n\n            def f_x2Px(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1))) * np.array(args)[0]**2\n\n            def f_xPx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1))) * np.array(args)[0]\n\n            def f_xRxPx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args)) * np.array(args)[0]\n            # result = integrate.quad(f_p,x_min,x_max,epsabs=epsabs,epsrel=epsrel)[0]*self.num_training_points\n            Ep = integrate.quad(f_p, x_min, x_max,\n                                epsabs=epsabs, epsrel=epsrel)[0]\n            Ex2 = integrate.quad(f_x2Px, x_min, x_max,\n                                 epsabs=epsabs, epsrel=epsrel)[0]\n            Ex_2 = integrate.quad(f_xPx, x_min, x_max,\n                                  epsabs=epsabs, epsrel=epsrel)[0] ** 2\n            result = Ex2 / Ep - Ex_2 / Ep / Ep\n\n        # if self.dimension > 1:\n        #     data_range_length_half = [(max(self.training_data.features[:, i])-min(\n        #         self.training_data.features[:, i]))*0.5 for i in range(self.dimension)]\n        #     data_range = [[min(self.training_data.features[:, i])-data_range_length_half[i], max(\n        # self.training_data.features[:, i])+data_range_length_half[i]] for i\n        # in range(self.dimension)]\n\n        #     # generate the integral bounds\n        #     bounds = []\n        #     for i in range(x_columnID):\n        #         bounds.append(data_range[i])\n        #     bounds.append([x_min, x_max])\n        #     # print(bounds)\n        #     for i in range(x_columnID+1, self.dimension):\n        #         bounds.append(data_range[i])\n\n        #     def f_p(*args):\n        #         return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n        #     result = integrate.nquad(f_p, bounds, opts=opts)[0]*self.num_training_points\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate variance x: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate variance x: %.4fs.\" % time_cost)\n        return result, time_cost\n\n    def approximate_variance_y_from_to(self, x_min=-np.inf, x_max=np.inf, x_columnID=0):\n        start = datetime.now()\n        if self.dimension is 1:\n            # average = self.approximate_ave_from_to(x_min,x_max,x_columnID)\n            def f_p(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n\n            def f_R2Px(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1))) * self.cregression.predict(np.array(args))**2\n\n            def f_RxPx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1))) * self.cregression.predict(np.array(args))\n            # result = integrate.quad(f_p,x_min,x_max,epsabs=epsabs,epsrel=epsrel)[0]*self.num_training_points\n            Ep = integrate.quad(f_p, x_min, x_max,\n                                epsabs=epsabs, epsrel=epsrel)[0]\n            Ey2 = integrate.quad(f_R2Px, x_min, x_max,\n                                 epsabs=epsabs, epsrel=epsrel)[0]\n            Ey_2 = integrate.quad(f_RxPx, x_min, x_max,\n                                  epsabs=epsabs, epsrel=epsrel)[0] ** 2\n            result = Ey2 / Ep - Ey_2 / Ep / Ep\n\n        # if self.dimension > 1:\n        #     data_range_length_half = [(max(self.training_data.features[:, i])-min(\n        #         self.training_data.features[:, i]))*0.5 for i in range(self.dimension)]\n        #     data_range = [[min(self.training_data.features[:, i])-data_range_length_half[i], max(\n        # self.training_data.features[:, i])+data_range_length_half[i]] for i\n        # in range(self.dimension)]\n\n        #     # generate the integral bounds\n        #     bounds = []\n        #     for i in range(x_columnID):\n        #         bounds.append(data_range[i])\n        #     bounds.append([x_min, x_max])\n        #     # print(bounds)\n        #     for i in range(x_columnID+1, self.dimension):\n        #         bounds.append(data_range[i])\n\n        #     def f_p(*args):\n        #         return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n        #     result = integrate.nquad(f_p, bounds, opts=opts)[0]*self.num_training_points\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate variance y: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate variance y: %.4fs.\" % time_cost)\n            if result < 0:\n                self.logger.warning(\n                    \"Negtive approximate variance y: %.4f. is predicted...\" % result)\n        return result, time_cost\n\n    def approximate_covar_from_to(self, x_min=-np.inf, x_max=np.inf, x_columnID=0):\n        start = datetime.now()\n        if self.dimension is 1:\n            # average = self.approximate_ave_from_to(x_min,x_max,x_columnID)\n            def f_p(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n\n            def f_px(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1))) * np.array(args)[0]\n\n            def f_pRx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args))\n\n            def f_xRxPx(*args):\n                return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\\\n                    * self.cregression.predict(np.array(args)) * np.array(args)[0]\n            # result = integrate.quad(f_p,x_min,x_max,epsabs=epsabs,epsrel=epsrel)[0]*self.num_training_points\n            Ep = integrate.quad(f_p, x_min, x_max,\n                                epsabs=epsabs, epsrel=epsrel)[0]\n            ExPx = integrate.quad(f_px, x_min, x_max,\n                                  epsabs=epsabs, epsrel=epsrel)[0]\n            ERxPx = integrate.quad(f_pRx, x_min, x_max,\n                                   epsabs=epsabs, epsrel=epsrel)[0]\n            ExRxPx = integrate.quad(\n                f_xRxPx, x_min, x_max, epsabs=epsabs, epsrel=epsrel)[0]\n            result = ExRxPx / Ep - ExPx * ERxPx / Ep / Ep\n\n        # if self.dimension > 1:\n        #     data_range_length_half = [(max(self.training_data.features[:, i])-min(\n        #         self.training_data.features[:, i]))*0.5 for i in range(self.dimension)]\n        #     data_range = [[min(self.training_data.features[:, i])-data_range_length_half[i], max(\n        # self.training_data.features[:, i])+data_range_length_half[i]] for i\n        # in range(self.dimension)]\n\n        #     # generate the integral bounds\n        #     bounds = []\n        #     for i in range(x_columnID):\n        #         bounds.append(data_range[i])\n        #     bounds.append([x_min, x_max])\n        #     # print(bounds)\n        #     for i in range(x_columnID+1, self.dimension):\n        #         bounds.append(data_range[i])\n\n        #     def f_p(*args):\n        #         return np.exp(self.kde.score_samples(np.array(args).reshape(1, -1)))\n        #     result = integrate.nquad(f_p, bounds, opts=opts)[0]*self.num_training_points\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\"Approximate COVAR: %.4f.\" % result)\n            self.logger.info(\n                \"Time spent for approximate COVAR: %.4fs.\" % time_cost)\n        return result, time_cost\n\n    def approximate_corr_from_to(self, x_min=-np.inf, x_max=np.inf, x_columnID=0):\n        start = datetime.now()\n        tmp_b = self.b_print_time_cost\n        self.b_print_time_cost = False\n        if self.dimension is 1:\n            var_x, _ = self.approximate_variance_x_from_to(\n                x_min=x_min, x_max=x_max, x_columnID=1)\n            var_y, _ = self.approximate_variance_y_from_to(\n                x_min=x_min, x_max=x_max, x_columnID=1)\n            if (var_x >= 0) and (var_y >= 0):\n                var_x = var_x**0.5\n                var_y = var_y**0.5\n                result = self.approximate_covar_from_to(\n                    x_min=x_min, x_max=x_max, x_columnID=1)[0] / var_x / var_y\n                self.logger.info(\"Approximate CORR: %.4f.\" % result)\n            else:\n                result = None\n                self.logger.warning(\n                    \"Cant be divided by zero! see Function approximate_corr_from_to()\")\n\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        self.b_print_time_cost = tmp_b\n        if self.b_print_time_cost:\n            self.logger.info(\n                \"Time spent for approximate CORR: %.4fs.\" % time_cost)\n        return result, time_cost\n\n    def approximate_percentile_from_to(self, p, q_min_boundary, q_max_boundary, x_columnID=0):\n        start = datetime.now()\n        result = generate_random.percentile(\n            p, self.kde, q_min_boundary, q_max_boundary, steps=500, n_bisect=20)\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        if self.b_print_time_cost:\n            self.logger.info(\n                \"Time spent for approximate CORR: %.4fs.\" % time_cost)\n        return result[0], time_cost\n\n\nif __name__ == \"__main__\":\n    import generate_random\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n    logger = logs.QueryLogs()\n    logger.set_no_output()\n    data = dl.load2d(5)\n    cRegression = CRegression(logger_object=logger)\n    cRegression.fit(data)\n\n    # cRegression.plot_training_data_2d()\n    logger.set_logging()\n    qe = QueryEngine(cRegression, logger_object=logger)\n    qe.density_estimation()\n    # qe.desngity_estimation_plt2d()\n\n    r = generate_random.percentile(\n        0.9, qe.kde, 30, 100, steps=200, n_bisect=100)\n    print(r)\n    # plt.plot(r)\n    # plt.show()\n\n    # qe.desngity_estimation_plt3d()\n    # qe.approximate_avg_from_to(70, 80, 0)[0]\n    # qe.approximate_sum_from_to(70, 80, 0)[0]\n    # qe.approximate_count_from_to(70, 80, 0)[0]\n    # qe.approximate_variance_x_from_to(70, 80, 0)[0]\n    # qe.approximate_variance_y_from_to(70, 80, 0)[0]\n    # qe.approximate_covar_from_to(70, 80, 0)[0]\n    #\n    # qe.approximate_avg_from_to(0.6, 0.8, 0)[0]\n    # qe.approximate_sum_from_to(0.6, 0.8, 0)[0]\n    # qe.approximate_count_from_to(0.6, 0.8, 0)[0]\n    # qe.approximate_variance_x_from_to(0.6, 0.8, 0)[0]\n    # qe.approximate_variance_y_from_to(0.6, 0.8, 0)[0]\n    # qe.approximate_covar_from_to(0.6, 0.8, 0)[0]\n    qe.approximate_corr_from_to(50, 70, 0)[0]\n\n    # qe.desngity_estimation_plt2d()\n",
			"file": "creg/query_engine.py",
			"file_size": 22071,
			"file_write_time": 131769906731936882,
			"settings":
			{
				"buffer_size": 22042,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "#!/usr/bin/env python\n# coding=utf-8\nfrom core import CRegression\nimport data_loader as dl\nfrom query_engine import QueryEngine\nimport logs\nimport random\nimport subprocess\n# from pyhive import hive\n# import subprocess\nimport os\nimport pyhs2\n# import MySQLdb\nimport pymysql\npymysql.install_as_MySQLdb()\nimport generate_random\n\n\nfrom datetime import datetime\nimport warnings\n\ndefault_mass_query_number = 5\nlogger_file = \"../results/deletable.log\"\n\n\nclass Query_Engine_2d:\n\n    def __init__(self, dataID, b_allow_repeated_value=True, logger_file=logger_file, num_of_points=None):\n        self.logger = logs.QueryLogs(log=logger_file)\n        # self.logger.set_no_output()\n        self.data = dl.load2d(dataID)\n        if not b_allow_repeated_value:\n            self.data.remove_repeated_x_1d()\n        self.cRegression = CRegression(logger_object=self.logger)\n        self.cRegression.fit(self.data)\n        # self.logger.set_logging(file_name=logger_file)\n        if num_of_points is None:\n            self.qe = QueryEngine(self.cRegression, logger_object=self.logger)\n        else:\n            self.qe = QueryEngine(\n                self.cRegression, logger_object=self.logger, num_training_points=num_of_points)\n        self.qe.density_estimation()\n        self.q_min = min(self.data.features)\n        self.q_max = max(self.data.features)\n        self.dataID = dataID\n\n        #warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\n    def query_2d_avg(self, l=0, h=100):\n        \"\"\"query to 2d data sets.\n\n        Args:\n            l (int, optional): query lower boundary\n            h (int, optional): query higher boundary\n        \"\"\"\n        avgs, time = self.qe.approximate_avg_from_to(l, h, 0)  # 0.05E8,0.1E8,\n        return avgs, time\n\n    def query_2d_sum(self, l=0, h=100):\n        \"\"\"query to 2d data sets.\n\n        Args:\n            l (int, optional): query lower boundary\n            h (int, optional): query higher boundary\n        \"\"\"\n        sums, time = self.qe.approximate_sum_from_to(l, h, 0)\n        return sums, time\n\n    def query_2d_count(self, l=0, h=100):\n        count, time = self.qe.approximate_count_from_to(l, h, 0)\n        return count, time\n\n    def query_2d_variance_x(self, l=0, h=100):\n        variance_x, time = self.qe.approximate_variance_x_from_to(l, h, 0)\n        return variance_x, time\n\n    def query_2d_variance_y(self, l=0, h=100):\n        variance_y, time = self.qe.approximate_variance_y_from_to(l, h, 0)\n        return variance_y, time\n\n    def query_2d_covariance(self, l=0, h=100):\n        covariance, time = self.qe.approximate_covar_from_to(l, h, 0)\n        return covariance, time\n\n    def query_2d_correlation(self, l=0, h=100):\n        correlation, time = self.qe.approximate_corr_from_to(l, h, 0)\n        return correlation, time\n\n    def query_2d_percentile(self, p):\n        percentile, time = self.qe.approximate_percentile_from_to(\n            p, self.q_min, self.q_max)\n        return percentile, time\n\n    def mass_query_sum(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        random.seed(1.0)\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            q_centre = random.uniform(self.q_min, self.q_max)\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT SUM(\" + y + \") FROM \" + str(table) + \" WHERE  \" + \\\n                x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_sum(l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_avg(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        random.seed(1.0)\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            q_centre = random.uniform(self.q_min, self.q_max)\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT AVG(\" + y + \") FROM \" + str(table) + \" WHERE  \" + \\\n                x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_avg(l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_count(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        random.seed(1.0)\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            q_centre = random.uniform(self.q_min, self.q_max)\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT COUNT(\" + y + \") FROM \" + str(table) + \" WHERE  \" + \\\n                x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_count(\n                l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_variance_x(self, table, x=\"ss_list_price\", \n        y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,\n        b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        self.logger.logger.info(\"Start generating queries\")\n        if b_random_queries:\n            random.seed(1.0)\n            query_centres = []\n            for i in range(number):\n                query_centres.append(random.uniform(self.q_min,self.q_max)[0])\n        else:\n            query_centres = generate_random.make_user_distribution(\n                self.qe.kde, self.q_min, self.q_max, n=number)\n        self.logger.logger.info(\"Finish generating \"+str(number)+\" queries, the center points are:\")\n        self.logger.logger.info(str(query_centres))\n        random_left_boundary = self.q_min + q_range_half_length\n        random_right_boundary = self.q_max - q_range_half_length\n        \n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            # random.uniform(self.q_min,self.q_max)\n            q_centre = query_centres[i]\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT VARIANCE(\" + x + \") FROM \" + str(table) + \" WHERE  \" + \\\n                x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_variance_x(\n                l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_variance_y(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        # random.seed(1.0)\n        random_left_boundary = self.q_min + q_range_half_length\n        random_right_boundary = self.q_max - q_range_half_length\n        self.logger.logger.info(\"Start generating queries\")\n        if b_random_queries:\n            random.seed(1.0)\n            query_centres = []\n            for i in range(number):\n                query_centres.append(random.uniform(self.q_min,self.q_max)[0])\n        else:\n            query_centres = generate_random.make_user_distribution(\n                self.qe.kde, self.q_min, self.q_max, n=number)\n        self.logger.logger.info(\"Finish generating \"+str(number)+\" queries, the center points are:\")\n        self.logger.logger.info(str(query_centres))\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            # random.uniform(self.q_min,self.q_max)\n            q_centre = query_centres[i]\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT VARIANCE(\" + y + \") FROM \" + str(table) + \" WHERE  \" + \\\n                x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_variance_y(\n                l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_covariance(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        # random.seed(1.0)\n        random_left_boundary = self.q_min + q_range_half_length\n        random_right_boundary = self.q_max - q_range_half_length\n        self.logger.logger.info(\"Start generating queries\")\n        if b_random_queries:\n            random.seed(1.0)\n            query_centres = []\n            for i in range(number):\n                query_centres.append(random.uniform(self.q_min,self.q_max)[0])\n        else:\n            query_centres = generate_random.make_user_distribution(\n                self.qe.kde, self.q_min, self.q_max, n=number)\n        self.logger.logger.info(\"Finish generating \"+str(number)+\" queries, the center points are:\")\n        self.logger.logger.info(str(query_centres))\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            # random.uniform(self.q_min,self.q_max)\n            q_centre = query_centres[i]\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT COVARIANCE(\" + x + \", \" + y + \") FROM \" + str(\n                table) + \" WHERE  \" + x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_covariance(\n                l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_correlation(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\",\n        percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        # random.seed(1.0)\n        random_left_boundary = self.q_min + q_range_half_length\n        random_right_boundary = self.q_max - q_range_half_length\n        self.logger.logger.info(\"Start generating queries\")\n        if b_random_queries:\n            random.seed(1.0)\n            query_centres = []\n            for i in range(number):\n                query_centres.append(random.uniform(self.q_min,self.q_max)[0])\n        else:\n            query_centres = generate_random.make_user_distribution(\n                self.qe.kde, self.q_min, self.q_max, n=number)\n        self.logger.logger.info(\"Finish generating \"+str(number)+\" queries, the center points are:\")\n        self.logger.logger.info(str(query_centres))\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            # random.uniform(self.q_min,self.q_max)\n            q_centre = query_centres[i]\n            q_left = q_centre - q_range_half_length\n            q_right = q_centre + q_range_half_length\n\n            sqlStr = \"SELECT CORR(\" + x + \", \" + y + \") FROM \" + str(\n                table) + \" WHERE  \" + x + \" BETWEEN \" + str(q_left[0]) + \" AND \" + str(q_right[0])\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_correlation(\n                l=q_left, h=q_right)\n            self.logger.logger.info(approx_result)\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        self.logger.logger.info(\"\")\n        return exact_results, approx_results, exact_times, approx_times\n\n    def mass_query_percentile(self, table, x=\"ss_list_price\", y=\"ss_wholesale_cost\", percent=5, number=default_mass_query_number,b_random_queries=True):\n        q_range_half_length = (self.q_max - self.q_min) * percent / 100.0 / 2.0\n        random.seed(1.0)\n\n        exact_results = []\n        exact_times = []\n        approx_results = []\n        approx_times = []\n        for i in range(number):\n            self.logger.logger.info(\n                \"start query No.\" + str(i + 1) + \" out of \" + str(number))\n            q_centre = random.uniform(0, 1)\n            sqlStr = \"SELECT percentile_cont(\" + x + \\\n                \", \" + str(q_centre) + \") FROM \" + str(table)\n            self.logger.logger.info(sqlStr)\n\n            approx_result, approx_time = self.query_2d_percentile(q_centre)\n            self.logger.logger.info(approx_result)\n\n            exact_result, exact_time = self.query2mysql(sql=sqlStr)\n\n            self.logger.logger.info(exact_result)\n            if (exact_result is not None) and (exact_result is not 0):\n                exact_results.append(exact_result)\n                exact_times.append(exact_time)\n                approx_results.append(approx_result)\n                approx_times.append(approx_time)\n            else:\n                self.logger.logger.warning(\n                    \"MYSQL returns None, so this record is ignored.\")\n        self.logger.logger.warning(\n            \"MYSQL query results: \" + str(exact_results))\n        self.logger.logger.warning(\n            \"MYSQL query time cost: \" + str(exact_times))\n        self.logger.logger.warning(\n            \"Approximate query results: \" + str(approx_results))\n        self.logger.logger.warning(\n            \"Approximate query time cost: \" + str(approx_times))\n        self.logger.logger.info(\"\")\n        return exact_results, approx_results, exact_times, approx_times\n\n    def relative_error(self, exact_results, approx_results):\n        # abs_errors = [abs(i - j) for i, j in zip(exact_results,approx_results) ]\n        rel_errors = [abs(i - j) * 1.0 / i for i,\n                      j in zip(exact_results, approx_results)]\n        # abs_time_reduction = [(j - i) for i, j in zip(exact_times, approx_times) ]\n        result = sum(rel_errors) / len(rel_errors)\n        self.logger.logger.warning(\"Relative error is : \" + str(result))\n        return result\n\n    def time_ratio(self, exact_times, approx_times):\n        result = sum(approx_times) / sum(exact_times)\n        self.logger.logger.warning(\"Time ratio is : \" + str(result))\n        return result\n\n    def query2hive(self, sql=\"SHOW TABLES\", use_server=True):\n        if use_server:\n            host = \"137.205.118.65\"\n        else:\n            host = \"localhost\"\n\n        with pyhs2.connect(host=host,\n                           port=10000,\n                           authMechanism=\"PLAIN\",\n                           user='hiveuser',\n                           password='bayern',\n                           database='default') as conn:\n            with conn.cursor() as cur:\n                # Show databases\n                # print cur.getDatabases()\n\n                # Execute query\n                # cur.execute(\"select * from src\")\n                start = datetime.now()\n                cur.execute(sql)\n                end = datetime.now()\n                time_cost = (end - start).total_seconds()\n                # Return column info from query\n                # print cur.getSchema()\n\n                # Fetch table results\n                self.logger.logger.info(\n                    \"Time spent for HIVE query: %.4fs.\" % time_cost)\n                for i in cur.fetch():\n                    self.logger.logger.info(i)\n        return i[0], time_cost\n\n    def query2mysql(self, sql=\"SHOW TABLES\", use_server=True):\n        # Open database connection\n        # db = MySQLdb.connect(\"127.0.0.1\",\"hiveuser\",\"bayern\",\"hivedb\" )\n        if use_server:\n            db = pymysql.connect(\"137.205.118.65\", \"u1796377\",\n                                 \"bayern\", \"hivedb\", port=3306)\n        else:\n            db = pymysql.connect(\"127.0.0.1\", \"hiveuser\",\n                                 \"bayern\", \"hivedb\", port=3306)\n\n        # prepare a cursor object using cursor() method\n        cursor = db.cursor()\n\n        # Drop table if it already exist using execute() method.\n        # cursor.execute(\"DROP TABLE IF EXISTS EMPLOYEE\")\n\n        # Create table as per requirement\n        # sql = sql\n        start = datetime.now()\n        cursor.execute(sql)\n        results = cursor.fetchall()\n\n        end = datetime.now()\n        time_cost = (end - start).total_seconds()\n        self.logger.logger.info(\n            \"Time spent for MYSQL query: %.4fs.\" % time_cost)\n        for row in results:\n            self.logger.logger.info(row)\n\n        # disconnect from server\n        db.close()\n        return row[0], time_cost\n\n    def mass_query(self, file, agg_func='percentile'):\n        AQP_results = []\n        with open(file) as fin:\n            for line in fin:\n                if agg_func is 'percentile':\n                    print(line)\n                    result, _ = self.query_2d_percentile(float(line))\n                    AQP_results.append(result)\n                    print(result)\n\n        print(AQP_results)\n        return AQP_results\n\n\n\n\nif __name__ == '__main__':\n\n    qe2d = Query_Engine_2d(\"100k\", num_of_points=100000,\n                           logger_file=\"../results/1m.log\")\n    qe2d.logger.set_level(\"DEBUG\")\n    qe2d.mass_query(file=\"percentile.hiveql\")\n\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_sum()\n    #\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_avg(table=\"price_cost_1t_sorted\",number=5,percent=1)\n    # qe2d.relative_error(exact_results, approx_results)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_count(table=\"price_cost_1t_sorted\",number=5,percent=1)\n    # qe2d.relative_error(exact_results, approx_results)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_sum(table=\"price_cost_1t_sorted\",number=5,percent=1)\n    # qe2d.relative_error(exact_results, approx_results)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_variance_x(table=\"price_cost_100k\",number=5,percent=1)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_variance_y(table=\"price_cost_100k\",number=5,percent=1)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_covariance(table=\"price_cost_100k\",number=5,percent=1)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_correlation(table=\"price_cost_100k\",number=5,percent=1)\n    # exact_results, approx_results, exact_times, approx_times = qe2d.mass_query_percentile(\n        # table=\"price_cost_100k\", number=5, percent=1)\n    # qe2d.relative_error(exact_results, approx_results)\n",
			"file": "creg/evaluate2d.py",
			"file_size": 26929,
			"file_write_time": 131769906731976882,
			"settings":
			{
				"buffer_size": 26963,
				"line_ending": "Unix"
			}
		},
		{
			"file": "creg/core.py",
			"settings":
			{
				"buffer_size": 117472,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "query/load_data/dataset8.sql",
			"settings":
			{
				"buffer_size": 1247,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		},
		{
			"file": "query/load_data/tcp_ds.sql",
			"settings":
			{
				"buffer_size": 6287,
				"encoding": "UTF-8",
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 392.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"PEP8",
				"Anaconda: Autoformat PEP8 Errors"
			],
			[
				"install ",
				"Package Control: Install Package"
			],
			[
				"install",
				"Package Control: Install Package"
			],
			[
				"outlien",
				"Browse Mode: Outline (Right)"
			],
			[
				"out",
				"Browse Mode: Outline (Right)"
			],
			[
				"in",
				"Package Control: Install Package"
			],
			[
				"re",
				"Package Control: Remove Package"
			],
			[
				"ins",
				"Package Control: Install Package"
			],
			[
				"doc",
				"AutoDocstring: Current"
			],
			[
				"do",
				"AutoDocstring: Current"
			],
			[
				"au",
				"AutoPEP8: Format Code"
			],
			[
				"outli",
				"Browse Mode: Outline (Right)"
			],
			[
				"remove",
				"Package Control: Remove Package"
			],
			[
				"auto",
				"AutoDocstring: Current"
			],
			[
				"fold",
				"Fold Python : fold content"
			],
			[
				"uni",
				"UnitTesting"
			],
			[
				"unit",
				"UnitTesting"
			],
			[
				"insta",
				"Package Control: Install Package"
			],
			[
				"bro",
				"Browse Mode: Outline (Right)"
			],
			[
				"package",
				"Install Package Control"
			]
		],
		"width": 524.0
	},
	"console":
	{
		"height": 302.0,
		"history":
		[
			"import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/home/u1796377/Workspace/CRegressionRDBM",
		"/home/u1796377/Workspace/CRegressionRDBM/creg",
		"/home/u1796377/Workspace/CRegressionRDBM/query",
		"/home/u1796377/Workspace/CRegressionRDBM/query/load_data",
		"/home/u1796377/Workspace/CRegressionRDBM/results"
	],
	"file_history":
	[
		"/tmp/rsub-4lhuua/results.log",
		"/tmp/rsub-dp3pq4/results.log",
		"/tmp/rsub-32kfe_/results.log",
		"/tmp/rsub-39ct0r/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-uewr3s/results.log.bak",
		"/tmp/rsub-hopoz7/results.log",
		"/tmp/rsub-scy8vh/results.log",
		"/tmp/rsub-dep5nl/results.log",
		"/tmp/rsub-tew2l1/results.log",
		"/tmp/rsub-tkme42/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-cgmce6/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-2rf4q1/results.log",
		"/tmp/rsub-o7s4fa/results.log",
		"/tmp/rsub-dhqrb0/results.log",
		"/tmp/rsub-qsr682/results.log",
		"/tmp/rsub-k9tikg/results.log",
		"/tmp/rsub-5z8t3r/results.log",
		"/tmp/rsub-i4syl7/results.log",
		"/tmp/rsub-zh3ybl/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-n39l_1/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-a34i68/1m_1_blinkdb_1m.hiveql",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m_1_percentile.log",
		"/home/u1796377/Desktop/1m_1_agge.hiveql",
		"/home/u1796377/Desktop/1m_1_aggregates.log",
		"/home/u1796377/Desktop/percentile.log",
		"/tmp/rsub-5avcq6/1m_1_blinkdb_1m_after365.hiveql",
		"/tmp/rsub-awzt_u/1m_1_blinkdb_1m_after365.hiveql",
		"/tmp/rsub-rioikz/000000_0",
		"/home/u1796377/Workspace/CRegressionRDBM/creg/deletable.py",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m_1.log",
		"/tmp/rsub-sp8l9l/blinkdb-env.sh",
		"/home/u1796377/Workspace/CRegressionRDBM/creg/percentile.hiveql",
		"/home/u1796377/Desktop/percentile.hiveql",
		"/home/u1796377/Program/blinkdb/conf/blinkdb-env.sh",
		"/tmp/rsub-lgwo9c/blinkdb-env.sh",
		"/opt/spark/spark1/conf/spark-env.sh",
		"/home/u1796377/Desktop/project2/results/blinkdb/1m_1.result",
		"/home/u1796377/Desktop/project2/results/blinkdb/1m_1.log",
		"/home/u1796377/Desktop/1m_1_380.log",
		"/home/u1796377/Desktop/1m_1.log",
		"/home/u1796377/Desktop/1m_1_380.result",
		"/home/u1796377/Desktop/1m_1.result",
		"/media/u1796377/sceavens Baby/2018-07-dcs-desktop/Desktop/project2/results/blinkdb/results.log",
		"/media/u1796377/sceavens Baby/2018-07-dcs-desktop/Desktop/project2/results/blinkdb/1m_1.log",
		"/home/u1796377/Desktop/result1m_1.log",
		"/home/u1796377/Desktop/project2/results/blinkdb/1m_1_blinkdb_1m.log",
		"/home/u1796377/Desktop/1m_1_blinkdb_1m_after365.log",
		"/home/u1796377/Desktop/result1m_2.log",
		"/home/u1796377/Desktop/project2/results/blinkdb/blinkdb_1m.result",
		"/home/u1796377/Desktop/project2/results/blinkdb/results.log",
		"/home/u1796377/Desktop/1m_1_blinkdb_1m.log",
		"/tmp/rsub-zbpabx/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-z262so/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-6qj44r/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-iz5t4r/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-ej68r6/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-73thu7/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-hcj6zg/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-hf7evy/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-_9x7wy/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-yoy69c/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-qmfoxe/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-m9xz8_/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-yvbuvn/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-kcjfhq/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-fr202w/1m_1_blinkdb_1m_after365.log",
		"/tmp/rsub-43gs2j/1m_1_blinkdb_1m.log",
		"/tmp/rsub-rqtpai/1m_1_blinkdb_1m_after365.hiveql",
		"/home/u1796377/Dropbox/symbol_to_system.sh",
		"/tmp/rsub-svyzlt/1m_1_blinkdb_1m.log",
		"/tmp/rsub-d0xbl2/1m_1_blinkdb_1m.log",
		"/tmp/rsub-x_a3b1/1m_1_blinkdb_1m.log",
		"/tmp/rsub-foouna/1m_1_blinkdb_1m.log",
		"/tmp/rsub-fah84u/1m_1_blinkdb_1m.log",
		"/tmp/rsub-ccghq6/1m_1_380.log",
		"/tmp/rsub-g5hqal/1m_1_blinkdb_1m.log",
		"/tmp/rsub-4olv20/mysqld.cnf",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m_1_blinkdb_1m.hiveql",
		"/tmp/rsub-ztbxj7/1m_1_blinkdb_1m.log",
		"/tmp/rsub-94ia49/1m_1_blinkdb_1m.log",
		"/tmp/rsub-y1_7ra/1m_1_blinkdb_1m.log",
		"/tmp/rsub-b1xznh/1m_1_blinkdb_1m.hiveql",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m_1_aggregates.log",
		"/tmp/rsub-aruou9/blinkdb-env.sh",
		"/home/u1796377/.bashrc",
		"/tmp/rsub-jil2i8/.bashrc",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/yarn-site.xml",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/yarn-env.sh",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/mapred-site.xml",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/mapred-env.sh",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/hdfs-site.xml",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/hadoop-env.sh",
		"/opt/hadoop/hadoop-2-9-0/etc/hadoop/core-site.xml",
		"/tmp/rsub-f3b4os/.bashrc",
		"/tmp/rsub-sedyxe/blinkdb-env.sh",
		"/opt/spark/conf/spark-env.sh",
		"/tmp/rsub-g7q2gf/.bashrc",
		"/home/u1796377/.ssh/config",
		"/tmp/rsub-z1zyz2/.bashrc",
		"/tmp/rsub-wb4d49/blinkdb-env.sh",
		"/tmp/rsub-k4i1oy/blinkdb-env.sh",
		"/tmp/rsub-1y139x/my.cnf",
		"/tmp/rsub-02slq2/mysql.cnf",
		"/tmp/rsub-5_mg2z/my.cnf",
		"/tmp/rsub-85_1xz/blinkdb-env.sh",
		"/tmp/rsub-5wfdn2/blinkdb-env.sh",
		"/tmp/rsub-c0ohuf/blinkdb-env.sh",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m.log",
		"/home/u1796377/.config/sublime-text-3/Packages/User/pep8_autoformat.sublime-settings",
		"/home/u1796377/.config/sublime-text-3/Packages/Python PEP8 Autoformat/pep8_autoformat.sublime-settings",
		"/home/u1796377/Workspace/CRegressionRDBM/results/1m_1_size.log",
		"/home/u1796377/Dropbox/backup.sh",
		"/home/u1796377/Workspace/CRegressionRDBM/creg/inverse_transform.py",
		"/usr/share/gnome-background-properties/bionic-wallpapers.xml",
		"/usr/share/gnome-shell/theme/ubuntu.css",
		"/home/u1796377/.config/sublime-text-3/Packages/User/python3_qz.sublime-build",
		"/home/u1796377/.config/sublime-text-3/Packages/User/Anaconda.sublime-settings",
		"/home/u1796377/.config/sublime-text-3/Packages/User/python3.sublime-build",
		"/home/u1796377/workspace/CRegressionRDBM/creg/tools.py",
		"/home/u1796377/workspace/CRegressionRDBM/requirements.txt",
		"/home/u1796377/workspace/CRegressionRDBM/query/load_data/tcp_ds.sql",
		"/home/u1796377/workspace/CRegressionRDBM/creg/core.py",
		"/home/u1796377/.config/sublime-text-3/Packages/User/python3_user.sublime-build",
		"/home/u1796377/.config/sublime-text-3/Packages/User/python2_user.sublime-build",
		"/home/u1796377/.config/sublime-text-3/Packages/Default/Preferences.sublime-settings",
		"/home/u1796377/workspace/CRegressionRDBM/creg/logs.py",
		"/home/u1796377/workspace/CRegressionRDBM/creg/query_engine.py",
		"/home/u1796377/workspace/CRegressionRDBM/creg/evaluate2d.py"
	],
	"find":
	{
		"height": 28.0
	},
	"find_in_files":
	{
		"height": 104.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"INFO - ",
			"price_cost_1t_sorted",
			"store_sales_sample_1m_cached",
			"merge",
			"head",
			")",
			" FROM price_cost_1t_sorted",
			"INFO - SELECT percentile_cont(ss_list_price, ",
			"confi",
			"store_sales_sample_1_percent_cached",
			"137.205.118.65",
			"Time to fit the model is",
			"\t",
			"jdk1.8.0_171",
			"jdk-10.0.1",
			"jdk-10.0.1_linux-x64_bin",
			"jdk1.8.0"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"",
			"whole_1m_cached",
			"",
			"store_sales_sample_1m_cached",
			"137.205.115.59",
			"    ",
			"jdk1.7.0_80",
			"jdk1.8.0_171",
			"jdk-10.0.1",
			"jdk-10.0.1_linux-x64_bin"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 0,
			"sheets":
			[
				{
					"buffer": 0,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 212,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"current_file": "/home/u1796377/Workspace/CRegressionRDBM/query/load_data/tcp_ds.sql",
							"git_gutter_is_enabled": false,
							"outline_rename_mode": false,
							"symkeys":
							[
								[
									1140,
									1142
								],
								[
									1242,
									1252
								],
								[
									1444,
									1457
								],
								[
									2039,
									2062
								],
								[
									2820,
									2840
								],
								[
									3240,
									3255
								],
								[
									3652,
									3669
								],
								[
									3787,
									3798
								],
								[
									5454,
									5482
								],
								[
									5542,
									5577
								],
								[
									6120,
									6148
								]
							],
							"symlist":
							[
								"xy",
								"price_cost",
								"price_cost_1t",
								"price_cost_1t_sample_5m",
								"price_cost_1t_sorted",
								"price_cost_100k",
								"idx_ss_list_price",
								"store_sales",
								"store_sales_sample_1_percent",
								"store_sales_sample_1_percent_cached",
								"store_sales_sample_1m_cached"
							],
							"syntax": "Packages/Outline/outline.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				}
			]
		},
		{
			"selected": 6,
			"sheets":
			[
				{
					"buffer": 1,
					"file": "creg/tools.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 34640,
						"regions":
						{
						},
						"selection":
						[
							[
								408,
								408
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "creg/generate_random.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1544,
						"regions":
						{
						},
						"selection":
						[
							[
								944,
								944
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"auto_name": "from __future__ import division",
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 183.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "creg/query_engine.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 22042,
						"regions":
						{
						},
						"selection":
						[
							[
								18792,
								18792
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 7475.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "creg/evaluate2d.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 26963,
						"regions":
						{
						},
						"selection":
						[
							[
								2501,
								2547
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1110.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "creg/core.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 117472,
						"regions":
						{
						},
						"selection":
						[
							[
								9415,
								9415
							]
						],
						"settings":
						{
							"auto_complete_triggers":
							[
								{
									"characters": ".",
									"selector": "source.python - string - comment - constant.numeric"
								},
								{
									"characters": ".",
									"selector": "source.python - string - constant.numeric"
								}
							],
							"git_gutter_is_enabled": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 4675.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "query/load_data/dataset8.sql",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1247,
						"regions":
						{
						},
						"selection":
						[
							[
								799,
								799
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/SQL/SQL.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "query/load_data/tcp_ds.sql",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6287,
						"regions":
						{
						},
						"selection":
						[
							[
								5125,
								5125
							]
						],
						"settings":
						{
							"git_gutter_is_enabled": true,
							"syntax": "Packages/SQL/SQL.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 2938.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 28.0
	},
	"input":
	{
		"height": 40.0
	},
	"layout":
	{
		"cells":
		[
			[
				2,
				0,
				3,
				2
			],
			[
				0,
				0,
				2,
				2
			]
		],
		"cols":
		[
			0.0,
			0.2,
			0.833803878796,
			1.0
		],
		"rows":
		[
			0.0,
			0.5,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 240.0
	},
	"output.find_results":
	{
		"height": 0.0
	},
	"output.mdpopups":
	{
		"height": 0.0
	},
	"pinned_build_system": "Packages/User/python3_qz.sublime-build",
	"project": "CRegressionRDBM.sublime-project",
	"replace":
	{
		"height": 52.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"tests/test_basic.py"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 1,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": true,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 243.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
